%Talk given virtually to Mac Hyman and friends, November 15, 2021
\documentclass[10pt,compress,xcolor={usenames,dvipsnames},aspectratio=169]{beamer}
%\documentclass[xcolor={usenames,dvipsnames},aspectratio=169]{beamer} %slides and 
%notes
\usepackage[T1]{fontenc}
\usepackage{tgadventor} %Font found at https://tug.org/FontCatalogue/
%\usepackage{newpxtext}
\usepackage[euler-digits,euler-hat-accent]{eulervm}

\usepackage{amsmath,
	amssymb,
	datetime,
	mathtools,
	bbm,
	%mathabx,
	array,
	booktabs,
	xspace,
	multirow,
	calc,
	colortbl,
	siunitx,
 	graphicx}
\usepackage[usenames]{xcolor}
\usepackage[giveninits=false,backend=biber,style=nature, maxcitenames =10, mincitenames=9]{biblatex}
\addbibresource{FJHown23.bib}
\addbibresource{FJH23.bib}
\usepackage{media9}
\usepackage[autolinebreaks]{mcode}
\usepackage[tikz]{mdframed}


\usetheme{FJHSlimNoFoot169}
\setlength{\parskip}{2ex}
\setlength{\arraycolsep}{0.5ex}



\usepackage{algpseudocode}
\usepackage{algorithm, algorithmicx}
\algnewcommand\algorithmicproblem{\textbf{Problem:}}
\algnewcommand\PROB{\item[\algorithmicproblem]}
\algnewcommand\algorithmicparam{\textbf{Parameters:}}
\algnewcommand\PARAM{\item[\algorithmicparam]}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\RETURN{\State \textbf{Return }}



\DeclareMathOperator{\COST}{COST}
\DeclareMathOperator{\STOP}{STOP}
\DeclareMathOperator{\DATA}{DATA}
\DeclareMathOperator{\SOL}{SOL}
\DeclareMathOperator{\OUT}{OUT}
\DeclareMathOperator{\CRIT}{CRIT}
\DeclareMathOperator{\APP}{APP}
\DeclareMathOperator{\ERR}{ERR}
\DeclareMathOperator{\AVG}{AVG}
\DeclareMathOperator{\INT}{INT}
\DeclareMathOperator{\LIN}{LINEAR}
\DeclareMathOperator{\BAD}{BAD}
%\DeclareMathOperator{\opt}{opt}
\newcommand{\dataN}{\bigl(\hf(\vk_i)\bigr)_{i=1}^n}
\newcommand{\dataNj}{\bigl(\hf(\vk_i)\bigr)_{i=1}^{n_j}}
\newcommand{\dataNjd}{\bigl(\hf(\vk_i)\bigr)_{i=1}^{n_{j^\dagger}}}
\newcommand{\ERRN}{\ERR\bigl(\dataN,n\bigr)}
\newcommand{\otod}{\ensuremath{1\mkern-4mu : \mkern-2mu d}}
\newcommand{\ttrue}{\text{true}}
\newcommand{\tfalse}{\text{false}}
\newcommand{\tolabs}{\varepsilon_{\text{a}}}
\newcommand{\tolrel}{\varepsilon_{\text{r}}}


%\DeclareMathOperator{\app}{app}

\providecommand{\HickernellFJ}{H.\xspace}


\renewcommand{\OffTitleLength}{-7ex}
\setlength{\FJHThankYouMessageOffset}{-8ex}
\title{Complexity of Numerical Problems}
\author[]{GAIL/QMCPy/GAAPy Group}
\institute{Department of Applied Mathematics \qquad
	Center for Interdisciplinary Scientific Computation \\
	Illinois Institute of Technology \qquad
	\href{mailto:hickernell@iit.edu}{\url{hickernell@iit.edu}} \qquad
	\href{http://mypages.iit.edu/~hickernell}{\url{mypages.iit.edu/~hickernell}}}

\thanksnote{}
	
\date[]{ revised \today}

\input FJHDef.tex



\begin{document}
	\everymath{\displaystyle}

\frame{\titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Mathematical Problems}

\vspace{-5ex}
Define a (continuous) \alert{mathematical problem} as follows:
\begin{align*}
    \cf & = \text{set of input functions defined on the domain } \Omega\\
    \cg & = \text{set of possible solutions} \\
    \SOL :\cf \to \cg &= \text{a solution operator}
\end{align*}
Examples include integration and root-finding
\begin{align*}
   \cf &= \{f \in C[0,1] : \norm[2]{f''} \le 1\} & \cf &= \{f \in C[0,1] : f(0) f(1) \le 0 \}\\
    \cg & = \reals &  \cg &= \text{set of all closed subsets of } [0,1]\\
    \SOL(f) &= \int_0^1 f(x) \, \dif x & \SOL(f) & = f^{-1}(0)
\end{align*}
Mathematical problems, $\SOL:\cf \to \cg$, are concerned with \alert{existence} or \alert{analytical solutions}.

We want to find a good \alert{numerical solution}
\end{frame}


\begin{frame}{Numerical Problems}

\vspace{-5ex}
Numerical solution, $\OUT$, should be be close to true solution, $\SOL(f)$, according to an error criterion, $\CRIT$, and tolerance, $\veps$:
\begin{align*}
    \OUT & = \text{numerical solution} \in \cg \\
    \veps & = \text{scalar or vector of error tolerances} \in \ce \subseteq [0,\infty)^s\\
    \CRIT: \cg \times \cg \times \ce \to \{\ttrue, \tfalse\}& = \text{error criterion} \\
\intertext{\alert{Acceptable numerical solutions} satisfy $\CRIT(\OUT,\SOL(f),\veps) = \ttrue$. \endgraf
In addition to knowledge about $f$ based on the definition of $\cf$, the algorithm constructing the numerical solution samples $f$, e.g., by generating function values or  derivative values:}
    \Lambda: \cf \to \reals^t &= \text{set of allowable \alert{data operators}}
\end{align*}
A \alert{numerical problem} is described by the ordered triple $(\SOL, \CRIT, \Lambda)$.

Next we look at \alert{algorithms} to solve numerical problems.
\end{frame}

\section{Algorithms}

\begin{frame}{Data Accumulation}

\vspace{-4ex}

Now we discuss the components of  an algorithm to solve a numerical problem $(\SOL, \CRIT, \Lambda)$, which is assumed fixed.

Algorithms must accumulate data about the input function by applying a sequence of data operators.  The \alert{data accumulation operator}
\[
\DATA: \cf \times  \ce \times\cup_{n \in \natzero} (\Lambda^n \times \reals^{n\times t} ) \to  \cup_{n \in \naturals} (\Lambda^n \times \reals^{n\times t}) 
\]
is defined iteratively.  Let $\mZ_0 = (\emptyset, \emptyset)$ and $\mZ_n \in \Lambda^n \times \reals^{n\times t}$.  Then,
\[
\DATA(f,\veps, \mZ_0) = \begin{pmatrix} L_{1} & L_{1}(f) \\ \vdots & \vdots \\ L_{n_1} & L_{n_1}(f) \end{pmatrix} = \mZ_{n_1}, \quad 
\DATA(f, \veps,\mZ_{n_1}) = 
\begin{pmatrix}\mZ_{n_1} \\ \begin{pmatrix} L_{n_1+1} & L_{n_1+1}(f) \\ \vdots & \vdots \\ L_{n_2} & L_{n_2}(f) \end{pmatrix}  \end{pmatrix} = \mZ_{n_2}, \quad \ldots
\]
where $0 < n_1 < n_2 < \cdots$, and $n_{\ell+1}$ depends on $\mZ_{n_\ell}$.  

\end{frame}


\begin{frame}{Design}
	
	

$\vL_n = (L_1, \ldots, L_n)^T$ is the \alert{design}, which may be \alert{adaptive}.
	
When the set of possible data operators, $\Lambda$, consists of function evaluations, then we may represent the design by the data sites, $\mX_n = (\vx_1, \ldots, \vx_n)^T \in \Omega^n \subseteq \reals^{n \times d}$.


\[
\DATA(f,\veps,\mZ_{n_{\ell-1}}) = \mZ_{n_{\ell}} = \bigl(\mX_{n_{\ell}}, f(\mX_{n_{\ell}}) \bigr)  \in  \reals^{n_\ell \times (d+t)}
\]

	
\end{frame}


\begin{frame}{Stopping Criteria}
	
The \alert{data based} stopping criterion
\[
\STOP:  \ce  \times \cup_{n \in \natzero} (\Lambda^n \times \reals^{n\times t} ) \to \cg \times \{\ttrue, \tfalse \}
\]
determines whether there exists an output, $\OUT$, that satisfies the error criterion, $\CRIT(\OUT,\SOL(f),\veps)$.   Given the function data $\mZ = (\vL,\mY)$,  construct 
\[
\cg(\mZ) \supseteq \{\SOL(f) :   f \in \cf, \ \vL(f) = \mY   \} = \text{ set of solutions consistent with function data}
\]
The data based stopping criterion is defined by
\[
\STOP(\veps,\mZ) = \begin{cases} 
	(\OUT,\ttrue), & 
	\exists \OUT \text{ such that } \CRIT(\OUT,g,\veps) = \ttrue \quad \forall g \in \cg(\mZ), \\
	(\OUT,\tfalse), & \text{otherwise ($\OUT$ may be the ``best'' so far or nil)}
	\end{cases}
\]

\end{frame}


\begin{frame}{Trapezpoidal Rule Integration}
For the integration problem above with a \alert{compound error criterion}
\begin{gather*}
	\cf = \{f \in C[0,1] : \norm[2]{f''} \le 1\}, \qquad 
	\cg  = \reals,  \qquad
	\SOL(f) = \int_0^1 f(x) \, \dif x  \\
	\CRIT(\OUT, \SOL(f),(\tolabs, \tolrel))  = \begin{cases} \ttrue, & \abs{\SOL(f) - \OUT} \le \max(\tolabs,\tolrel \abs{\SOL(f)}), \\
		\tfalse, & \text{otherwise}, 
		\end{cases} \\
	 \Lambda  = \{L :  L(f) = f(x) \text{ for some } x \in [0,1]\} = \text{function evaluations}
\end{gather*}
The following data accumulation and stopping criterion will succeed
\begin{gather*}
\end{gather*}


\end{frame}


\begin{frame}{Algorithm}
	
\vspace{-2ex}
\begin{algorithm}[H]
	\renewcommand{\thealgorithm}{}
	\caption{General Algorithm Pattern}
	\begin{algorithmic}
		\PROB $(\SOL, \CRIT,\Lambda)$
		\INPUT black-box function, $f$;  error tolerance, $\veps$
		
		\Ensure  $\CRIT(\OUT,\SOL(f),\veps) = \ttrue$
		
		\State $\mZ \leftarrow (\emptyset, \emptyset)$
				
		\Repeat 
		
		\State $\mZ \leftarrow \DATA(f,\mZ)$

		\State $(\OUT,\text{done}) \leftarrow \STOP(\veps,\mZ)$
				
		\Until{$\text{done} = \ttrue$}
		
		\State $n  \leftarrow \text{\# rows in } \mZ$
		
		\RETURN $\OUT$, $n$
	\end{algorithmic}
\end{algorithm}	
\end{frame}


\end{document}


\begin{frame}{Bayesian Cubature Enhancements}

Investigators: Fred Hickernell, Jagadeeswaran Rathinavel, Claude Hall

\url{https://www.overleaf.com/project/5ded59976c7d0b00011f57d6}

\end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notes, Related Preprints, or Numerical Experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}{Duality Between Reproducing Kernels and Measures}
Investigators: Fred Hickernell, Claude Hall

\url{https://www.overleaf.com/2459845262nsvdsvhrxqwp}
    
    If you have a reproducing kernel, $K$, then it seems to induce an inner product on a space of (distributions of) measures.
\end{frame}


\begin{frame}{Professor Kai-Tai Fang's Problem}

Investigators: 

\url{https://www.overleaf.com/1816161655sqmpvyqsbshs}
   
\end{frame}


\begin{frame}{Convergence Rate for Variable Transformations of QMC Points}

Investigators: Claude Hall

\url{https://www.overleaf.com/5133543839fmgfwqbtmrbv}
    
    A smooth integrand integrated against the Gaussian distribution does not seem to exhibit $\Order(n^{-3/2})$ convergence using scrambled Sobol'
    
    
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dreams}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Optimal Continuous Design for RKHS Function Approximation}

Investigators: 

\url{}
   
\end{frame}


\begin{frame}{Bayesian Cubature for Higher Order Nets}
   
Investigators: 

\end{frame}


\begin{frame}{QMC/quasirandom nonparametric statistics}
   
Investigators: Mike McCourt

What sort of statements can be said about, e.g., medians and percentiles of distributions sampled quasirandomly (instead of the mean, which is the integral)?  Art wrote this ... maybe related?  https://arxiv.org/pdf/2111.12676.pdf
If we can say anything about this, it would be cool


\end{frame}


\begin{frame}{GAAPY Prototype}


Investigators: Sou-Cheng Choi, Yuhan Ding, Fred Hickernell, Xin Tong

\url{}


\end{frame}



\begin{frame}{Parametric Integration}


Investigators: Fred Hickernell

\url{https://www.overleaf.com/9869163733ggdzybqpxyrm}


\end{frame}


\end{document}


\chapter{Various Error Criteria} \label{chap:relerror}
The error criterion in \cref{prob:findzerocont} requires that the location of one zero of the function be identified within an interval of half-width $\varepsilon$.  The true solution of $f(x) = 0$ can be written as the set $f^{-1}(0)$.  If the zero-finding algorithm outputs $x_0$, then the true error of the output is $\min_{x \in f^{-1}(0)} \abs{x-x_0}$. The absolute error criterion in \cref{prob:findzerocont} that the output, $x_0$, must satisfy can be expressed as 
\begin{equation} \label{eq:fzeroerrorcrit}
    \crit\bigl(x_0,f^{-1}(0),\varepsilon\bigr) = 
    \begin{cases} \true, & \exists x \in f^{-1}(0) \text{ such that } \abs{x-x_0} \le \varepsilon,\\
    \false, & \text{otherwise}.
    \end{cases}
\end{equation}

In practice, it is not possible to know the true error of the output, but \cref{alg:zeroBisectionAB} iteratively refines the interval $[x_{\text{left}}, x_{\text{right}}]$ that must contain at least one zero of $f$. 
The data-driven stopping criterion for \cref{alg:zeroBisectionAB} may be expressed by replacing the true solution by the all possible partial solutions as
\begin{equation}
    \datacrit\bigl(x_0,[x_{\text{left}},x_{\text{right}}],\varepsilon\bigr) = 
    \begin{cases} \true, & \crit\bigl(x_0,\{x\},\varepsilon\bigr) = \true \ \ \forall x \in [x_{\text{left}},x_{\text{right}}] ,\\
    \false, & \text{otherwise},
    \end{cases}
\end{equation}

Choosing $x_0 = (x_{\text{left}}+x_{\text{right}})/2$ as is done in \cref{alg:zeroBisectionAB} gives the stopping criterion the best chance of being $\true$.  When $\datacrit\bigl(x_0,[x_{\text{left}},x_{\text{right}}],\varepsilon\bigr) = \true$, then \cref{alg:zeroBisectionAB} stops and returns $x_0$.

In some cases, the practitioner may wish to choose a different kind of error criterion than an absolute one, such as a relative error criterion or one involving a combination of absolute and relative error tolerances.  In such cases, choosing the middle of the interval containing the solution may be not be best.  In this chapter we explore various error criteria, their corresponding stopping criteria, and the choices of outputs.

\section{A Generic Numerical Problem and Template Algorithm}

Let's generalize the zero-finding situation to a generic numerical problem.  As before, we let $\cf$ denote the set of input functions, which must be large enough so that the successful algorithm is widely applicable but constrained so that a successful algorithm exists. 

\begin{NumProblem}[Generic Numerical Problem]
\label{prob:generalProblem}
\problemspecs{set of functions $\cf$ \\ 
solution operator $\sol: \cf \to \cg$}
{black-box function $f \in \cf$ \\ error tolerance (vector) $\veps$}
{$\out \in \ca$ such that \\ \qquad $\crit(\out,\sol(f),\veps) = \true$}
\end{NumProblem}

The solution operator, $\sol$, maps an element in $\cf$ to a \emph{set} of acceptable solutions, not a single solution. This is to allow for the case of non-unique acceptable solutions. For example, if $f: x \mapsto x^3 - x$ defined on $[-2,2]$, then $\sol(f) = \{-1, 0, 1\}$ for zero-finding \cref{prob:findzerocont}, where only one zero is required, but  $\sol(f) = \{ \{-1, 0, 1\} \}$ for \cref{prob:findzerocontall}, where all zeros are required.  The set of sets of acceptable solutions is denoted $\cg$.

The output required from the problem comes from the set of acceptable solutions $\ca$.  The elements of $\ca$ are the elements of the elements of $\cg$, i.e., $\ca = \{S \in G : G \in \cg\}$.  For $f: x \mapsto x^3 - x$ defined on $[-2,2]$ in zero-finding \cref{prob:findzerocont}, $\out$ is a number sufficiently close to $-1$ or $0$ or $1$.  For \cref{prob:findzerocontall}, $\out$ for this same function is a set sufficiently close to  $\{-1, 0, 1\}$.  The sets $\cf$, $\cg$, and $\ca$ are tabulated for different problems in \cref{tab:genericproblem}.  

\begin{table}[H]
    \centering
    \caption{Examples of specific problems following Generic \cref{prob:generalProblem}}
      {\small \begin{tabular}{c>{\centering}m{0.21\textwidth}>{\centering}m{0.15\textwidth}>{\centering}m{0.25\textwidth}>{\centering}m{0.2\textwidth}}
       \multicolumn{2}{c}{Problem} & Inputs, $\cf$ & $\cg$ & $\ca$
         \tabularnewline \toprule
         \ref{prob:findzerocont} & 
         $\sol(f) = f^{-1}(0)$ & $\bigcup_{a < b} C[a,b]$ \\ $f(a)f(b) \le 0$ & closed, bounded subsets of $\reals$ & $\reals$
        \tabularnewline \midrule
        \ref{prob:findzerocontall} &
        $\sol(f) = \{f^{-1}(0)\}$ & $\bigcup_{a < b} C[a,b]$ & sets with a single element: a closed, bounded subset of $\reals$  & 
        closed, bounded subsets of $\reals$
        \tabularnewline \bottomrule
        \end{tabular} }
    \label{tab:genericproblem}
\end{table}

The error criterion, $\crit: \ca \times \cg \times (0,\infty)^s \to \{\true, \false\}$, specifies how close the output must be to an element in $\sol(f)$.  The first input is an algorithm output, the second input is the true solution, and the third input is a (vector) error tolerance.  The error criterion for the zero-finding problem defined in \eqref{eq:fzeroerrorcrit} is an example.  Various error criteria are discussed in this chapter.

Next we define a template \cref{alg:template} for \cref{prob:generalProblem}.  The algorithm works with $\cs$, a set of acceptable solutions consistent with the function data.  We initialize $\cs$ to be $\ca$, the whole set of acceptable solutions, which is also are set of possible outputs.
At each iteration, the algorithm samples $f$ further and shrinks $\cs$ to be consistent with all function data.  A data-based stopping criterion is defined by determining whether the error criterion for the problem is true for all possible acceptable solutions in $\cs$:
\begin{equation}
    \datacrit(\out,\cs,\veps) = \begin{cases} \true, & \crit(\out,S,\veps) = \true \quad \forall S \in \cs, \\ \false, & \text{otherwise} .
    \end{cases}
\end{equation}
When $\cs$ becomes small enough so that an output exists that $\out$ exists that makes the stopping criterion $\true$, then the algorithm terminates.

\begin{algorithm}[H]
\caption{Template algorithm for Generic Numerical \cref{prob:generalProblem} \label{alg:template}}
	\begin{algorithmic}
   \State Initialize $\cs$, the set containing acceptable solutions consistent with function data, to be $\ca$
   \State Shrink $\cs$ based on an initial sample of $f$
    \Repeat 
    \State Shrink $\cs$ further based on further sampling of $f$
    \State Choose $\out$ optimally
    \Until $\datacrit(\out,\cs,\varepsilon) = \true$
    \RETURN $\out$
    \end{algorithmic}
\end{algorithm}



\section{Absolute Error Criterion} \label{sec:abserror}
The error criterion for the zero-finding problem defined in \eqref{eq:fzeroerrorcrit}  is an absolute error criterion, which requires that the output be within $\varepsilon$ of the true answer. The following is a more general absolute error criterion where the set of acceptable solutions, $\ca$, is a subset of the real numbers: 
\begin{equation} \label{eq:absnormerrorcrit}
    \crit(\out,\sol(f),\varepsilon) = 
    \begin{cases} \true, & \exists S \in \sol(f) \text{ with } \abs{\out-S} \le \varepsilon,\\
    \false, & \text{otherwise}.
    \end{cases}
\end{equation}

For the If $\cs$, the set of acceptable solutions consistent with the function data in \cref{alg:template}, is the interval $\cs = [S_{\lo}, S_{\high}]$, then the optimal choice of $\out$ is the center of the interval.

\cref{tab:errorcrit} shows the optimal choices of the output, $\out$ for various error criteria that can be expressed in terms of the absolute value between the output and the solution and where an acceptable true solution is known to be in $[S_{\lo}, S_{\high}]$.  In the following sections we explore relative and compound error criteria.  In these cases, the interval midpoint may not be the optimal choice for $\out$.


\begin{table}[H]
    \centering
    \caption{Conditions under which the error criterion is true and the corresponding optimal output when an acceptable true solution lies in $\cs = [S_{\lo}, S_{\high}]$.}
    \begin{equation*}
       \begin{array}{ccc}
         \crit(\out,\sol(f),\varepsilon) && \text{Sufficient condition for}\\
         \exists S \in \sol(f) \text{ such that}
         & \out 
         & \datacrit(\out,\cs,\varepsilon) = \true \tabularnewline \toprule
        \abs{\out-S} \le \varepsilon & 
        (S_{\lo} + S_{\high})/2 &
        S_{\high} - S_{\lo} < 2\varepsilon \\
         \midrule
         \abs{\out-S} \le \varepsilon \abs{G} 
         & 
        \end{array} 
    \end{equation*}
    
    \label{tab:errorcrit}
\end{table}

\section{Relative Error}
Sometimes the error requirement is relative to the size of the answer, e.g., 
\begin{equation} \label{eq:absnormerrorcrit}
    \crit(\out,\sol(f),\varepsilon) = 
    \begin{cases} \true, & \exists S \in \sol(f) \text{ with } \abs{\out-S} \le \varepsilon \abs{S},\\
    \false, & \text{otherwise}.
    \end{cases}
\end{equation}
Here we assume that the relative error tolerance, $\varepsilon$. is strictly less than one.
The relative error criterion in \eqref{eq:absnormerrorcrit} is true if
\begin{equation}
    \min_{\out \in \ca} g(\out) \le 0, \quad \text{where } g(t) = \max_{S_{\lo} \le S \le S_{\high}} h(t,S), \quad h(t,S) = \abs{t - S} - \varepsilon \abs{S},
\end{equation}
where the set of solutions consistent with the function data is $[S_{\lo}, S_{\high}]$.
Note that it is impossible to satisfy this sufficient condition if $S_{\lo}S_{\high} \le 0$.

The function $h(t,\cdot)$ is piecewise linear with a slope corresponding to $\sign(\cdot-t)$ since $\varepsilon < 1$.  Thus, $h(t,\cdot)$ obtains its maximum at $S_{\lo}$ or $S_{\high}$, i.e., $g(t)$ corresponds to $h(t,S_{\lo})$ or $h(t,S_{\high})$.  The functions $h(\cdot,S_{\lo})$ and $h(\cdot,S_{\high})$ are also piececwise linear.  Thus, the value of $t$ that  minimizes $g(t)$ is the one for which $h(t,S_{\lo}) = h(t,S_{\high})$.  This optimal value of $\out$ satisfies
\begin{gather*}
    \abs{\out - S_{\lo}} - \varepsilon \abs{S_{\lo}} = \abs{\out - S_{\high}} - \varepsilon \abs{S_{\high}}
\end{gather*}